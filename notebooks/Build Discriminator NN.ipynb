{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Activation, Bidirectional, Embedding, GRU, Concatenate\n",
    "from keras.layers.core import Reshape, Dropout\n",
    "from keras.models import Model, Input, save_model, load_model\n",
    "\n",
    "# Params\n",
    "MAX_SEQUENCE_LENGTH = 20000\n",
    "DISC_HIDDEN_SIZE_LSTM = 64\n",
    "DISC_HIDDEN_SIZE_DENSE = 4612\n",
    "dropout = 0.2\n",
    "weight_decay = 0.25\n",
    "samples_per_epoch = 12000\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layer_q = Embedding(\n",
    "            input_dim=20000,\n",
    "            output_dim=300,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "\n",
    "embeddings_layer_d = Embedding(\n",
    "            input_dim=20000,\n",
    "            output_dim=300,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import zip\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"AdamW optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay (L2 penalty) (default: 0.025).\n",
    "        batch_size: integer >= 1. Batch size used during training.\n",
    "        samples_per_epoch: integer >= 1. Number of samples (training points) per epoch.\n",
    "        epochs: integer >= 1. Total number of epochs for training.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0.025,\n",
    "                 batch_size=1, samples_per_epoch=1,\n",
    "                 epochs=1, **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.batch_size = K.variable(batch_size, name='batch_size')\n",
    "            self.samples_per_epoch = K.variable(samples_per_epoch, name='samples_per_epoch')\n",
    "            self.epochs = K.variable(epochs, name='epochs')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        '''Bias corrections according to the Adam paper\n",
    "        '''\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            '''Schedule multiplier eta_t = 1 for simple AdamW\n",
    "            According to the AdamW paper, eta_t can be fixed, decay, or \n",
    "            also be used for warm restarts (AdamWR to come). \n",
    "            '''\n",
    "            eta_t = 1.\n",
    "            p_t = p - eta_t * (lr_t * m_t / (K.sqrt(v_t) + self.epsilon))\n",
    "            if self.weight_decay != 0:\n",
    "                '''Normalized weight decay according to the AdamW paper\n",
    "                '''\n",
    "                w_d = self.weight_decay * K.sqrt(self.batch_size / (self.samples_per_epoch * self.epochs))\n",
    "                p_t = p_t - eta_t * (w_d * p)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "                  'batch_size': int(K.get_value(self.batch_size)),\n",
    "                  'samples_per_epoch': int(K.get_value(self.samples_per_epoch)),\n",
    "                  'epochs': int(K.get_value(self.epochs)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamw = AdamW(batch_size=8, samples_per_epoch=samples_per_epoch,\n",
    "                      epochs=12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_query_6:0' shape=(?, 20000) dtype=int32>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input_q = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_query')\n",
    "sequence_input_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7f64869341d0>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sequences_q = embeddings_layer_q(sequence_input_q)\n",
    "embeddings_layer_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_29/concat:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_q_in = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=True, activation='elu', dropout=dropout, recurrent_dropout=dropout))(embedded_sequences_q)\n",
    "lstm_q_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_30/concat:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_q_out = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=False, activation='elu', dropout=dropout, recurrent_dropout=dropout))(lstm_q_in)\n",
    "lstm_q_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_doc_6:0' shape=(?, 20000) dtype=int32>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input_d = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_doc')\n",
    "sequence_input_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_14/embedding_lookup:0' shape=(?, 20000, 300) dtype=float32>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sequences_d = embeddings_layer_d(sequence_input_d)\n",
    "embedded_sequences_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_31/concat:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_d_in = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=True, activation='elu', dropout=dropout, recurrent_dropout=dropout))(embedded_sequences_d)\n",
    "lstm_d_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_32/concat:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_d_out = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=False, activation='elu', dropout=dropout, recurrent_dropout=dropout))(lstm_d_in)\n",
    "lstm_d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate and then Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_8/concat:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Concatenate()([lstm_q_out, lstm_d_out])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_8/cond/Merge:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dropout(dropout)(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'merged_input_7/Elu:0' shape=(?, 4612) dtype=float32>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(DISC_HIDDEN_SIZE_DENSE,\n",
    "          activation='elu',\n",
    "          kernel_regularizer=regularizers.l2(),\n",
    "          kernel_initializer=initializers.random_normal(stddev=0.01),\n",
    "          name='merged_input')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_8/Elu:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(1, activation='elu')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_7/Reshape:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = Reshape([-1])(x)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = Activation('sigmoid', name='prob')(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f6483925f60>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[sequence_input_q, sequence_input_d], outputs=[prob])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_query (InputLayer)        (None, 20000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_doc (InputLayer)          (None, 20000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 20000, 300)   6000000     input_query[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 20000, 300)   6000000     input_doc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_29 (Bidirectional (None, 20000, 128)   140160      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_31 (Bidirectional (None, 20000, 128)   140160      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_30 (Bidirectional (None, 128)          74112       bidirectional_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_32 (Bidirectional (None, 128)          74112       bidirectional_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           bidirectional_30[0][0]           \n",
      "                                                                 bidirectional_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merged_input (Dense)            (None, 4612)         1185284     dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            4613        merged_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1)            0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 1)            0           reshape_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,618,441\n",
      "Trainable params: 1,618,441\n",
      "Non-trainable params: 12,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f6483925f60>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=adamw,\n",
    "                      metrics=['accuracy'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_query_6:0' shape=(?, 20000) dtype=int32>, <tf.Tensor 'input_doc_6:0' shape=(?, 20000) dtype=int32>]\n",
      "Tensor(\"prob_6/Sigmoid:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inp = model.input\n",
    "print(inp)\n",
    "out = model.get_layer(\"prob\").output\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize queries\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found 1247 unique tokens.\n",
      "Tokenize documents\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found 7709 unique tokens.\n",
      "Found 1 training data.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "WORKDIR = '/home/lukas/git-projects/lstm-irgan'\n",
    "DOCUMENTS_DIR = WORKDIR + '/data/wikiclir/dev.docs'  #'/data/example/documents/'\n",
    "QUERIES = WORKDIR + '/data/wikiclir/dev.queries' #'/data/example/queries.txt'\n",
    "LABELLED_DATA = WORKDIR + '/data/wikiclir/dev.qrel' #'/data/example/labelled_data.txt'\n",
    "\n",
    "def __get_documents():\n",
    "    path = DOCUMENTS_DIR\n",
    "    documents = {}\n",
    "    doc_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            documents[id] = text\n",
    "            doc_ids.append(id)\n",
    "    return documents, doc_ids\n",
    "\n",
    "\n",
    "def __get_queries():\n",
    "    path = QUERIES\n",
    "    queries = {}\n",
    "    query_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            queries[id] = text\n",
    "            query_ids.append(id)\n",
    "    return queries, query_ids\n",
    "\n",
    "\n",
    "def __get_ratings():\n",
    "    path = LABELLED_DATA\n",
    "    ratings = {}\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\")\n",
    "            query = int(values[0])\n",
    "            text = int(values[2])\n",
    "            rating = float(values[3])\n",
    "\n",
    "            if query in ratings.keys():\n",
    "                ratings[query][text] = rating\n",
    "            else:\n",
    "                ratings[query] = {text: rating}\n",
    "\n",
    "    return ratings\n",
    "\n",
    "\n",
    "def __filter_stop_words(texts, stop_words):\n",
    "    for i, text in enumerate(texts):\n",
    "        new_text = [word for word in text.split() if word not in stop_words]\n",
    "        texts[i] = ' '.join(new_text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def __init_tokenizer(text_data, max_sequence_length):\n",
    "    texts = list(text_data.values())\n",
    "    ids = list(text_data.keys())\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "    texts = __filter_stop_words(texts, stop_words)\n",
    "\n",
    "    # finally, vectorize the text samples into a 2D integer tensor\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    text_data_sequenced = {}\n",
    "    for i, text in enumerate(data):\n",
    "        text_data_sequenced[ids[i]] = text\n",
    "\n",
    "    return tokenizer, text_data_sequenced\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    documents_data, doc_ids = __get_documents()\n",
    "    queries_data, query_ids = __get_queries()\n",
    "    ratings_data = __get_ratings()\n",
    "\n",
    "    print('Tokenize queries')\n",
    "    tokenizer_q, queries_data = __init_tokenizer(queries_data, MAX_SEQUENCE_LENGTH)\n",
    "    print('Tokenize documents')\n",
    "    tokenizer_d, documents_data = __init_tokenizer(documents_data, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    print('Found %s training data.' % len(ratings_data))\n",
    "\n",
    "    return query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d\n",
    "\n",
    "query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   0,    0,    0, ...,   23, 3388, 1496], dtype=int32), array([   0,    0,    0, ...,  286,  591, 2098], dtype=int32), array([  0,   0,   0, ..., 169, 503, 188], dtype=int32)]\n",
      "[array([  0,   0,   0, ...,   2, 204,  28], dtype=int32), array([  0,   0,   0, ...,   2, 204,  28], dtype=int32), array([  0,   0,   0, ...,   2, 204,  28], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "docs = [value for key, value in list(documents_data.items())[:3]]\n",
    "key, value = list(queries_data.items())[0]\n",
    "queries = [value]*3\n",
    "prob = [0.2, 0.5, 0,1]\n",
    "print(docs)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_query_6:0' shape=(?, 20000) dtype=int32>, <tf.Tensor 'input_doc_6:0' shape=(?, 20000) dtype=int32>, <tf.Tensor 'bidirectional_1/keras_learning_phase:0' shape=() dtype=bool>]\n",
      "<keras.backend.tensorflow_backend.Function object at 0x7f64855013c8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00151122],\n",
       "       [-0.00453931],\n",
       "       [-0.00172484]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_reward(train_data_queries, train_data_documents):\n",
    "    inputs = model.inputs + [K.learning_phase()]\n",
    "    print(inputs)\n",
    "    out = model.get_layer(\"prob\").output\n",
    "    functor = K.function(inputs, [out])\n",
    "    print(functor)\n",
    "    layer_outs = functor([train_data_queries, train_data_documents, 1.])\n",
    "    return (layer_outs[0] - 0.5) * 2\n",
    "\n",
    "reward = get_reward(queries, docs)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00094205],\n",
       "       [-0.00294149],\n",
       "       [-0.00079733]], dtype=float32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_preresult(train_data_queries, train_data_documents):\n",
    "    return (model.predict([train_data_queries, train_data_documents]) - 0.5) * 2\n",
    "    \n",
    "preresult = get_preresult(queries, docs)\n",
    "preresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(train_data_queries, train_data_documents, train_data_label):\n",
    "    model.train_on_batch([train_data_queries, train_data_documents], train_data_label)\n",
    "    \n",
    "# choose data\n",
    "choose_queries = np.array(queries)\n",
    "choose_documents = np.array(docs)\n",
    "\n",
    "# prepare pos and neg label\n",
    "pred_data_label = [1.0] * 1\n",
    "pred_data_label.extend([0.0] * 2)\n",
    "pred_data_label = np.asarray(pred_data_label)\n",
    "\n",
    "train(choose_queries, choose_documents, pred_data_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
