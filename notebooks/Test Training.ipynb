{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "WORKDIR = '/home/lukas/git-projects/lstm-irgan'\n",
    "DOCUMENTS_DIR = WORKDIR + '/data/wikiclir/dev.docs'  #'/data/example/documents/'\n",
    "QUERIES = WORKDIR + '/data/wikiclir/dev.queries' #'/data/example/queries.txt'\n",
    "LABELLED_DATA = WORKDIR + '/data/wikiclir/dev.qrel' #'/data/example/labelled_data.txt'\n",
    "\n",
    "def __get_documents():\n",
    "    path = DOCUMENTS_DIR\n",
    "    documents = {}\n",
    "    doc_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            documents[id] = text\n",
    "            doc_ids.append(id)\n",
    "    return documents, doc_ids\n",
    "\n",
    "\n",
    "def __get_queries():\n",
    "    path = QUERIES\n",
    "    queries = {}\n",
    "    query_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            queries[id] = text\n",
    "            query_ids.append(id)\n",
    "    return queries, query_ids\n",
    "\n",
    "\n",
    "def __get_ratings():\n",
    "    path = LABELLED_DATA\n",
    "    ratings = {}\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            values = line.split(\"\\t\")\n",
    "            query = int(values[0])\n",
    "            text = int(values[2])\n",
    "            rating = float(values[3])\n",
    "\n",
    "            if query in ratings.keys():\n",
    "                ratings[query][text] = rating\n",
    "            else:\n",
    "                ratings[query] = {text: rating}\n",
    "\n",
    "    return ratings\n",
    "\n",
    "\n",
    "def __filter_stop_words(texts, stop_words):\n",
    "    for i, text in enumerate(texts):\n",
    "        new_text = [word for word in text.split() if word not in stop_words]\n",
    "        texts[i] = ' '.join(new_text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def __init_tokenizer(text_data, max_sequence_length):\n",
    "    texts = list(text_data.values())\n",
    "    ids = list(text_data.keys())\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "    texts = __filter_stop_words(texts, stop_words)\n",
    "\n",
    "    # finally, vectorize the text samples into a 2D integer tensor\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    text_data_sequenced = {}\n",
    "    for i, text in enumerate(data):\n",
    "        text_data_sequenced[ids[i]] = text\n",
    "\n",
    "    return tokenizer, text_data_sequenced\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    documents_data, doc_ids = __get_documents()\n",
    "    queries_data, query_ids = __get_queries()\n",
    "    ratings_data = __get_ratings()\n",
    "\n",
    "    print('Tokenize queries')\n",
    "    tokenizer_q, queries_data = __init_tokenizer(queries_data, MAX_SEQUENCE_LENGTH)\n",
    "    print('Tokenize documents')\n",
    "    tokenizer_d, documents_data = __init_tokenizer(documents_data, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    print('Found %s training data.' % len(ratings_data))\n",
    "\n",
    "    return query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize queries\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found 41594 unique tokens.\n",
      "Tokenize documents\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = False #True\n",
    "tf_config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=tf_config)\n",
    "\n",
    "backend.set_session(sess)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #\"\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(query_ids, test_size=0.15, random_state=42)\n",
    "\n",
    "p_best_val = 0.0\n",
    "ndcg_best_val = 0.0\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __build_train_data(x_train, ratings_data, queries_data, documents_data):\n",
    "    train_queries_data = {}\n",
    "    train_documents_data = {}\n",
    "    train_ratings_data = {}\n",
    "\n",
    "    for query_id in x_train:\n",
    "        train_ratings_data[query_id] = ratings_data[query_id]\n",
    "        train_queries_data[query_id] = queries_data[query_id]\n",
    "        for key in ratings_data.keys():\n",
    "            if key in documents_data.keys():\n",
    "                train_documents_data[key] = documents_data[key]\n",
    "\n",
    "    return train_ratings_data, train_queries_data, train_documents_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_k, x_val_k = train_test_split(query_ids, test_size=0.50, random_state=42)\n",
    "\n",
    "train_ratings_data, train_queries_data, train_documents_data = __build_train_data(x_train, ratings_data, queries_data, documents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_query_specific_data(query_id, ratings_data, documents_data):\n",
    "    # get all query specific ratings\n",
    "    x_pos_list = list(ratings_data[query_id].keys())\n",
    "    y_pos_list = list(ratings_data[query_id].values())\n",
    "\n",
    "    # get all other ratings\n",
    "    docs_pos_ids = np.unique(x_pos_list)\n",
    "    candidate_list = []\n",
    "    for doc_id in documents_data.keys():\n",
    "        if doc_id not in docs_pos_ids:\n",
    "            candidate_list.append(doc_id)\n",
    "\n",
    "    return x_pos_list, y_pos_list, candidate_list\n",
    "\n",
    "def __get_rand_batch_from_candidates_for_negatives(query_id, queries_data, documents_data, candidate_list, x_pos_list):\n",
    "    rand_batch = np.random.choice(np.arange(len(candidate_list)), [5 * len(x_pos_list)])\n",
    "\n",
    "    # prepare pos and neg data\n",
    "    data_queries = [queries_data[query_id]] * len(rand_batch)\n",
    "    doc_ids = np.array(candidate_list)[rand_batch]\n",
    "    data_documents = [documents_data[x] for x in doc_ids]\n",
    "\n",
    "    # Importance Sampling\n",
    "    prob = [0.2,0.2,0.2]\n",
    "\n",
    "    return prob, data_queries, data_documents\n",
    "\n",
    "query_id = x_train[0]\n",
    "\n",
    "x_pos_list, y_pos_list, candidate_list = __get_query_specific_data(query_id, ratings_data, documents_data)\n",
    "\n",
    "prob, data_queries, data_documents = __get_rand_batch_from_candidates_for_negatives(query_id, queries_data, documents_data, candidate_list, x_pos_list)\n",
    "\n",
    "neg_list = np.random.choice(candidate_list, size=[len(x_pos_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "class FastText(object):\n",
    "    def __init__(self, fasttext_lib_directory, fasttext_model_path):\n",
    "        cmds = [fasttext_lib_directory, 'print-word-vectors', fasttext_model_path]  # Add '-' in the end for interactive mode, yet it didn't work for me...\n",
    "        self.model = subprocess.Popen(cmds, stdout=subprocess.PIPE, stdin=subprocess.PIPE, env=os.environ.copy())\n",
    "\n",
    "        # Test the model\n",
    "        print('\\nTesting the model...\\nPrediction for apple: ')\n",
    "        item = 'apple\\n'\n",
    "        item = item.encode('utf-8')\n",
    "        self.model.stdin.write(item)\n",
    "        result = self.model.stdout.readline()\n",
    "        result = result[len(item):]\n",
    "        result = np.fromstring(result, dtype=np.float32, sep=' ')\n",
    "        self.vector_size = len(result)\n",
    "        print('Length of word-vector is:', self.vector_size)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        assert type(item) is str\n",
    "        initial_item = item\n",
    "        item = item.lower().replace('/', '').replace('-', '').replace('\\\\', '').replace('`', '')\n",
    "        if len(item) == 0 or ' ' in item:\n",
    "            raise KeyError('Could not process: ' + initial_item)\n",
    "\n",
    "        if not item.endswith('\\n'):\n",
    "            item += '\\n'\n",
    "\n",
    "        item = item.encode('utf-8')\n",
    "        self.model.stdin.write(item)\n",
    "        self.model.stdout.flush()\n",
    "        result = self.model.stdout.readline()  # Read result\n",
    "        result = result[len(item):]            # Take everything but the initial item\n",
    "        result = np.fromstring(result, dtype=np.float32, sep=' ')\n",
    "\n",
    "        if len(result) != self.vector_size:\n",
    "            print('Could not process: ' + item)\n",
    "            raise KeyError('Could not process: ' + initial_item)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(fasttext_lib_directory='./fastText/fasttext', fasttext_model_path='/home/lukas/Downloads/BioWordVec_PubMed_MIMICIII_d200.bin')\n",
    "print(model['apple'])\n",
    "print(model['banana'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
