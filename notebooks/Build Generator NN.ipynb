{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Bidirectional, Embedding, GRU, Dense, Activation, Lambda, Concatenate\n",
    "from keras.layers.core import Reshape, Dropout\n",
    "from keras.models import Model, Input, save_model, load_model\n",
    "\n",
    "# Params\n",
    "MAX_SEQUENCE_LENGTH = 20000\n",
    "DISC_HIDDEN_SIZE_LSTM = 64\n",
    "DISC_HIDDEN_SIZE_DENSE = 46\n",
    "dropout = 0.2\n",
    "weight_decay = 0.25\n",
    "samples_per_epoch = 12000\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layer_q = Embedding(\n",
    "            input_dim=20000,\n",
    "            output_dim=300,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "\n",
    "embeddings_layer_d = Embedding(\n",
    "            input_dim=20000,\n",
    "            output_dim=300,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import zip\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"AdamW optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay (L2 penalty) (default: 0.025).\n",
    "        batch_size: integer >= 1. Batch size used during training.\n",
    "        samples_per_epoch: integer >= 1. Number of samples (training points) per epoch.\n",
    "        epochs: integer >= 1. Total number of epochs for training.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0.025,\n",
    "                 batch_size=1, samples_per_epoch=1,\n",
    "                 epochs=1, **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.batch_size = K.variable(batch_size, name='batch_size')\n",
    "            self.samples_per_epoch = K.variable(samples_per_epoch, name='samples_per_epoch')\n",
    "            self.epochs = K.variable(epochs, name='epochs')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        '''Bias corrections according to the Adam paper\n",
    "        '''\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            '''Schedule multiplier eta_t = 1 for simple AdamW\n",
    "            According to the AdamW paper, eta_t can be fixed, decay, or \n",
    "            also be used for warm restarts (AdamWR to come). \n",
    "            '''\n",
    "            eta_t = 1.\n",
    "            p_t = p - eta_t * (lr_t * m_t / (K.sqrt(v_t) + self.epsilon))\n",
    "            if self.weight_decay != 0:\n",
    "                '''Normalized weight decay according to the AdamW paper\n",
    "                '''\n",
    "                w_d = self.weight_decay * K.sqrt(self.batch_size / (self.samples_per_epoch * self.epochs))\n",
    "                p_t = p_t - eta_t * (w_d * p)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "                  'batch_size': int(K.get_value(self.batch_size)),\n",
    "                  'samples_per_epoch': int(K.get_value(self.samples_per_epoch)),\n",
    "                  'epochs': int(K.get_value(self.epochs)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamw = AdamW(batch_size=8, samples_per_epoch=samples_per_epoch,\n",
    "                      epochs=12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_reward:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = Input(shape=(None,), name='input_reward')\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_imp_sampling:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_sampling = Input(shape=(None,), name='input_imp_sampling')\n",
    "important_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_query:0' shape=(?, 20000) dtype=int32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input_q = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_query')\n",
    "sequence_input_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7fa76431ee48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sequences_q = embeddings_layer_q(sequence_input_q)\n",
    "embeddings_layer_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_1/concat:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_q_in = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=True, activation='elu', dropout=dropout, recurrent_dropout=dropout))(embedded_sequences_q)\n",
    "lstm_q_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_2/concat:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_q_out = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=False, activation='elu', dropout=dropout, recurrent_dropout=dropout))(lstm_q_in)\n",
    "lstm_q_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_doc:0' shape=(?, 20000) dtype=int32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input_d = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_doc')\n",
    "sequence_input_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_2/embedding_lookup:0' shape=(?, 20000, 300) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sequences_d = embeddings_layer_d(sequence_input_d)\n",
    "embedded_sequences_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_3/concat:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_d_in = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=True, activation='elu', dropout=dropout, recurrent_dropout=dropout))(embedded_sequences_d)\n",
    "lstm_d_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_4/concat:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_d_out = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=False, activation='elu', dropout=dropout, recurrent_dropout=dropout))(lstm_d_in)\n",
    "lstm_d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate and then Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_1/concat:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Concatenate()([lstm_q_out, lstm_d_out])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/cond/Merge:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dropout(dropout)(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/Elu:0' shape=(?, 46) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(DISC_HIDDEN_SIZE_DENSE,\n",
    "          activation='elu')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_2/Elu:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(1, activation='elu')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.00\n",
    "score = Lambda(lambda z: z / temperature, name='raw_score')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'score/Reshape:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = Reshape([-1], name='score')(score)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = Activation('softmax', name='prob')(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fa712bea588>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[sequence_input_q, sequence_input_d, reward, important_sampling], outputs=[prob])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_query (InputLayer)        (None, 20000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_doc (InputLayer)          (None, 20000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20000, 300)   6000000     input_query[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20000, 300)   6000000     input_doc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20000, 128)   140160      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 20000, 128)   140160      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          74112       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128)          74112       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 46)           11822       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            47          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "raw_score (Lambda)              (None, 1)            0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "score (Reshape)                 (None, 1)            0           raw_score[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 1)            0           score[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,440,413\n",
      "Trainable params: 440,413\n",
      "Non-trainable params: 12,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __loss(_reward, _important_sampling):\n",
    "    def _loss(y_true, y_pred):\n",
    "        log_action_prob = K.log(y_pred)\n",
    "        loss = - K.reshape(log_action_prob, [-1]) * K.reshape(_reward, [-1]) * K.reshape(_important_sampling, [-1])\n",
    "        loss = K.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fa712bea588>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=__loss(reward, important_sampling),\n",
    "                optimizer=adamw,\n",
    "                metrics=['accuracy'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='generator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"786pt\" viewBox=\"0.00 0.00 534.00 786.00\" width=\"534pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 782)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-782 530,-782 530,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140356917259512 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140356917259512</title>\n",
       "<polygon fill=\"none\" points=\"52.5,-741 52.5,-777 201.5,-777 201.5,-741 52.5,-741\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-755.3\">input_query: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140356917259848 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140356917259848</title>\n",
       "<polygon fill=\"none\" points=\"46.5,-667 46.5,-703 207.5,-703 207.5,-667 46.5,-667\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-681.3\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140356917259512&#45;&gt;140356917259848 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140356917259512-&gt;140356917259848</title>\n",
       "<path d=\"M127,-740.937C127,-732.807 127,-722.876 127,-713.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"130.5,-713.441 127,-703.441 123.5,-713.441 130.5,-713.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355775320976 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140355775320976</title>\n",
       "<polygon fill=\"none\" points=\"330,-741 330,-777 468,-777 468,-741 330,-741\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-755.3\">input_doc: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140356917259960 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140356917259960</title>\n",
       "<polygon fill=\"none\" points=\"318.5,-667 318.5,-703 479.5,-703 479.5,-667 318.5,-667\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-681.3\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 140355775320976&#45;&gt;140356917259960 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140355775320976-&gt;140356917259960</title>\n",
       "<path d=\"M399,-740.937C399,-732.807 399,-722.876 399,-713.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"402.5,-713.441 399,-703.441 395.5,-713.441 402.5,-713.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355804474896 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140355804474896</title>\n",
       "<polygon fill=\"none\" points=\"0,-593 0,-629 254,-629 254,-593 0,-593\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-607.3\">bidirectional_1(gru_1): Bidirectional(GRU)</text>\n",
       "</g>\n",
       "<!-- 140356917259848&#45;&gt;140355804474896 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140356917259848-&gt;140355804474896</title>\n",
       "<path d=\"M127,-666.937C127,-658.807 127,-648.876 127,-639.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"130.5,-639.441 127,-629.441 123.5,-639.441 130.5,-639.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355565466008 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140355565466008</title>\n",
       "<polygon fill=\"none\" points=\"272,-593 272,-629 526,-629 526,-593 272,-593\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-607.3\">bidirectional_3(gru_3): Bidirectional(GRU)</text>\n",
       "</g>\n",
       "<!-- 140356917259960&#45;&gt;140355565466008 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140356917259960-&gt;140355565466008</title>\n",
       "<path d=\"M399,-666.937C399,-658.807 399,-648.876 399,-639.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"402.5,-639.441 399,-629.441 395.5,-639.441 402.5,-639.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355773904152 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140355773904152</title>\n",
       "<polygon fill=\"none\" points=\"0,-519 0,-555 254,-555 254,-519 0,-519\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-533.3\">bidirectional_2(gru_2): Bidirectional(GRU)</text>\n",
       "</g>\n",
       "<!-- 140355804474896&#45;&gt;140355773904152 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140355804474896-&gt;140355773904152</title>\n",
       "<path d=\"M127,-592.937C127,-584.807 127,-574.876 127,-565.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"130.5,-565.441 127,-555.441 123.5,-565.441 130.5,-565.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355558285096 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140355558285096</title>\n",
       "<polygon fill=\"none\" points=\"272,-519 272,-555 526,-555 526,-519 272,-519\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-533.3\">bidirectional_4(gru_4): Bidirectional(GRU)</text>\n",
       "</g>\n",
       "<!-- 140355565466008&#45;&gt;140355558285096 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140355565466008-&gt;140355558285096</title>\n",
       "<path d=\"M399,-592.937C399,-584.807 399,-574.876 399,-565.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"402.5,-565.441 399,-555.441 395.5,-565.441 402.5,-565.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140357018743696 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140357018743696</title>\n",
       "<polygon fill=\"none\" points=\"179,-445 179,-481 347,-481 347,-445 179,-445\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-459.3\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140355773904152&#45;&gt;140357018743696 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140355773904152-&gt;140357018743696</title>\n",
       "<path d=\"M159.23,-518.937C177.894,-509.056 201.584,-496.514 221.615,-485.91\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"223.33,-488.962 230.53,-481.19 220.054,-482.776 223.33,-488.962\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355558285096&#45;&gt;140357018743696 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140355558285096-&gt;140357018743696</title>\n",
       "<path d=\"M366.77,-518.937C348.106,-509.056 324.416,-496.514 304.385,-485.91\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"305.946,-482.776 295.47,-481.19 302.67,-488.962 305.946,-482.776\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355552255728 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140355552255728</title>\n",
       "<polygon fill=\"none\" points=\"200.5,-371 200.5,-407 325.5,-407 325.5,-371 200.5,-371\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-385.3\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 140357018743696&#45;&gt;140355552255728 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140357018743696-&gt;140355552255728</title>\n",
       "<path d=\"M263,-444.937C263,-436.807 263,-426.876 263,-417.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"266.5,-417.441 263,-407.441 259.5,-417.441 266.5,-417.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355773904880 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140355773904880</title>\n",
       "<polygon fill=\"none\" points=\"212,-297 212,-333 314,-333 314,-297 212,-297\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-311.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140355552255728&#45;&gt;140355773904880 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140355552255728-&gt;140355773904880</title>\n",
       "<path d=\"M263,-370.937C263,-362.807 263,-352.876 263,-343.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"266.5,-343.441 263,-333.441 259.5,-343.441 266.5,-343.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355550887104 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140355550887104</title>\n",
       "<polygon fill=\"none\" points=\"212,-223 212,-259 314,-259 314,-223 212,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-237.3\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140355773904880&#45;&gt;140355550887104 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140355773904880-&gt;140355550887104</title>\n",
       "<path d=\"M263,-296.937C263,-288.807 263,-278.876 263,-269.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"266.5,-269.441 263,-259.441 259.5,-269.441 266.5,-269.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355551071144 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140355551071144</title>\n",
       "<polygon fill=\"none\" points=\"200.5,-149 200.5,-185 325.5,-185 325.5,-149 200.5,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-163.3\">raw_score: Lambda</text>\n",
       "</g>\n",
       "<!-- 140355550887104&#45;&gt;140355551071144 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140355550887104-&gt;140355551071144</title>\n",
       "<path d=\"M263,-222.937C263,-214.807 263,-204.876 263,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"266.5,-195.441 263,-185.441 259.5,-195.441 266.5,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355550634168 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140355550634168</title>\n",
       "<polygon fill=\"none\" points=\"214,-75 214,-111 312,-111 312,-75 214,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-89.3\">score: Reshape</text>\n",
       "</g>\n",
       "<!-- 140355551071144&#45;&gt;140355550634168 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140355551071144-&gt;140355550634168</title>\n",
       "<path d=\"M263,-148.937C263,-140.807 263,-130.876 263,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"266.5,-121.441 263,-111.441 259.5,-121.441 266.5,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140355550636072 -->\n",
       "<g class=\"node\" id=\"node15\"><title>140355550636072</title>\n",
       "<polygon fill=\"none\" points=\"210,-1 210,-37 316,-37 316,-1 210,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-15.3\">prob: Activation</text>\n",
       "</g>\n",
       "<!-- 140355550634168&#45;&gt;140355550636072 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>140355550634168-&gt;140355550636072</title>\n",
       "<path d=\"M263,-74.937C263,-66.8072 263,-56.8761 263,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"266.5,-47.4406 263,-37.4407 259.5,-47.4407 266.5,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_query_1:0' shape=(?, 20000) dtype=int32>, <tf.Tensor 'input_doc_1:0' shape=(?, 20000) dtype=int32>]\n",
      "Tensor(\"score_1/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"prob_1/Softmax:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inp = model.inputs[:2]\n",
    "print(inp)\n",
    "out_score = model.get_layer(\"score\").output\n",
    "print(out_score)\n",
    "out_prob = model.get_layer(\"prob\").output\n",
    "print(out_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize queries\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found 1247 unique tokens.\n",
      "Tokenize documents\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found 7709 unique tokens.\n",
      "Found 1 training data.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "WORKDIR = '/home/lukas/git-projects/lstm-irgan'\n",
    "DOCUMENTS_DIR = WORKDIR + '/data/wikiclir/dev.docs'  #'/data/example/documents/'\n",
    "QUERIES = WORKDIR + '/data/wikiclir/dev.queries' #'/data/example/queries.txt'\n",
    "LABELLED_DATA = WORKDIR + '/data/wikiclir/dev.qrel' #'/data/example/labelled_data.txt'\n",
    "\n",
    "def __get_documents():\n",
    "    path = DOCUMENTS_DIR\n",
    "    documents = {}\n",
    "    doc_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            documents[id] = text\n",
    "            doc_ids.append(id)\n",
    "    return documents, doc_ids\n",
    "\n",
    "\n",
    "def __get_queries():\n",
    "    path = QUERIES\n",
    "    queries = {}\n",
    "    query_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            queries[id] = text\n",
    "            query_ids.append(id)\n",
    "    return queries, query_ids\n",
    "\n",
    "\n",
    "def __get_ratings():\n",
    "    path = LABELLED_DATA\n",
    "    ratings = {}\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\")\n",
    "            query = int(values[0])\n",
    "            text = int(values[2])\n",
    "            rating = float(values[3])\n",
    "\n",
    "            if query in ratings.keys():\n",
    "                ratings[query][text] = rating\n",
    "            else:\n",
    "                ratings[query] = {text: rating}\n",
    "\n",
    "    return ratings\n",
    "\n",
    "\n",
    "def __filter_stop_words(texts, stop_words):\n",
    "    for i, text in enumerate(texts):\n",
    "        new_text = [word for word in text.split() if word not in stop_words]\n",
    "        texts[i] = ' '.join(new_text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def __init_tokenizer(text_data, max_sequence_length):\n",
    "    texts = list(text_data.values())\n",
    "    ids = list(text_data.keys())\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "    texts = __filter_stop_words(texts, stop_words)\n",
    "\n",
    "    # finally, vectorize the text samples into a 2D integer tensor\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    text_data_sequenced = {}\n",
    "    for i, text in enumerate(data):\n",
    "        text_data_sequenced[ids[i]] = text\n",
    "\n",
    "    return tokenizer, text_data_sequenced\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    documents_data, doc_ids = __get_documents()\n",
    "    queries_data, query_ids = __get_queries()\n",
    "    ratings_data = __get_ratings()\n",
    "\n",
    "    print('Tokenize queries')\n",
    "    tokenizer_q, queries_data = __init_tokenizer(queries_data, MAX_SEQUENCE_LENGTH)\n",
    "    print('Tokenize documents')\n",
    "    tokenizer_d, documents_data = __init_tokenizer(documents_data, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    print('Found %s training data.' % len(ratings_data))\n",
    "\n",
    "    return query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d\n",
    "\n",
    "query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   0,    0,    0, ...,   23, 3388, 1496], dtype=int32), array([   0,    0,    0, ...,  286,  591, 2098], dtype=int32), array([  0,   0,   0, ..., 169, 503, 188], dtype=int32)]\n",
      "[array([  0,   0,   0, ...,   2, 204,  28], dtype=int32), array([  0,   0,   0, ...,   2, 204,  28], dtype=int32), array([  0,   0,   0, ...,   2, 204,  28], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "docs = [value for key, value in list(documents_data.items())[:3]]\n",
    "key, value = list(queries_data.items())[0]\n",
    "queries = [value]*3\n",
    "print(docs)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00322543],\n",
       "        [0.00568766],\n",
       "        [0.00801787]], dtype=float32)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(train_data_queries, train_data_documents,):\n",
    "    inputs = model.inputs + [K.learning_phase()]\n",
    "    out = model.get_layer('score').output\n",
    "    functor = K.function(inputs, [out])\n",
    "    layer_outs = functor([train_data_queries, train_data_documents, 0.])\n",
    "    return layer_outs\n",
    "\n",
    "reward = get_score(queries, docs)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.],\n",
       "        [1.],\n",
       "        [1.]], dtype=float32)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prob(train_data_queries, train_data_documents,):\n",
    "    inputs = model.inputs + [K.learning_phase()]\n",
    "    out = model.get_layer('prob').output\n",
    "    functor = K.function(inputs, [out])\n",
    "    layer_outs = functor([train_data_queries, train_data_documents, 0.])\n",
    "    return layer_outs\n",
    "\n",
    "prob = get_prob(queries, docs)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(train_data_queries, train_data_documents, reward, important_sampling):\n",
    "    model.train_on_batch([train_data_queries, train_data_documents, reward, important_sampling], np.zeros([train_data_queries.shape[0]]))\n",
    "\n",
    "# choose data\n",
    "choose_queries = np.array(queries)\n",
    "choose_documents = np.array(docs)\n",
    "\n",
    "choose_queries = np.asarray(choose_queries)\n",
    "choose_documents = np.asarray(choose_documents)\n",
    "\n",
    "choose_reward = np.asarray([0.2, 0.5, 0.6])\n",
    "\n",
    "choose_is = np.asarray([0.2, 0.5, 0.6])\n",
    "\n",
    "train(choose_queries, choose_documents, choose_reward.reshape([-1]), choose_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
