{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Bidirectional, Embedding, GRU, Dense, Activation, Lambda, Concatenate\n",
    "from keras.layers.core import Reshape, Dropout\n",
    "from keras.models import Model, Input, save_model, load_model\n",
    "\n",
    "# Params\n",
    "MAX_SEQUENCE_LENGTH = 20000\n",
    "DISC_HIDDEN_SIZE_LSTM = 64\n",
    "DISC_HIDDEN_SIZE_DENSE = 4612\n",
    "dropout = 0.2\n",
    "weight_decay = 0.25\n",
    "samples_per_epoch = 12000\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layer_q = Embedding(\n",
    "            input_dim=20000,\n",
    "            output_dim=300,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "\n",
    "embeddings_layer_d = Embedding(\n",
    "            input_dim=20000,\n",
    "            output_dim=300,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import zip\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"AdamW optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay (L2 penalty) (default: 0.025).\n",
    "        batch_size: integer >= 1. Batch size used during training.\n",
    "        samples_per_epoch: integer >= 1. Number of samples (training points) per epoch.\n",
    "        epochs: integer >= 1. Total number of epochs for training.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0.025,\n",
    "                 batch_size=1, samples_per_epoch=1,\n",
    "                 epochs=1, **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.batch_size = K.variable(batch_size, name='batch_size')\n",
    "            self.samples_per_epoch = K.variable(samples_per_epoch, name='samples_per_epoch')\n",
    "            self.epochs = K.variable(epochs, name='epochs')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        '''Bias corrections according to the Adam paper\n",
    "        '''\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            '''Schedule multiplier eta_t = 1 for simple AdamW\n",
    "            According to the AdamW paper, eta_t can be fixed, decay, or \n",
    "            also be used for warm restarts (AdamWR to come). \n",
    "            '''\n",
    "            eta_t = 1.\n",
    "            p_t = p - eta_t * (lr_t * m_t / (K.sqrt(v_t) + self.epsilon))\n",
    "            if self.weight_decay != 0:\n",
    "                '''Normalized weight decay according to the AdamW paper\n",
    "                '''\n",
    "                w_d = self.weight_decay * K.sqrt(self.batch_size / (self.samples_per_epoch * self.epochs))\n",
    "                p_t = p_t - eta_t * (w_d * p)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "                  'batch_size': int(K.get_value(self.batch_size)),\n",
    "                  'samples_per_epoch': int(K.get_value(self.samples_per_epoch)),\n",
    "                  'epochs': int(K.get_value(self.epochs)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamw = AdamW(batch_size=8, samples_per_epoch=samples_per_epoch,\n",
    "                      epochs=12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_reward_1:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = Input(shape=(None,), name='input_reward')\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_imp_sampling:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_sampling = Input(shape=(None,), name='input_imp_sampling')\n",
    "important_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_query_1:0' shape=(?, 20000) dtype=int32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input_q = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_query')\n",
    "sequence_input_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7f283057fdd8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sequences_q = embeddings_layer_q(sequence_input_q)\n",
    "embeddings_layer_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_5/concat:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_q_in = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=True, activation='elu', dropout=dropout, recurrent_dropout=dropout))(embedded_sequences_q)\n",
    "lstm_q_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_6/concat:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_q_out = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=False, activation='elu', dropout=dropout, recurrent_dropout=dropout))(lstm_q_in)\n",
    "lstm_q_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_doc_1:0' shape=(?, 20000) dtype=int32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input_d = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_doc')\n",
    "sequence_input_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_2_1/embedding_lookup:0' shape=(?, 20000, 300) dtype=float32>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sequences_d = embeddings_layer_d(sequence_input_d)\n",
    "embedded_sequences_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_7/concat:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_d_in = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=True, activation='elu', dropout=dropout, recurrent_dropout=dropout))(embedded_sequences_d)\n",
    "lstm_d_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_8/concat:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_d_out = Bidirectional(GRU(DISC_HIDDEN_SIZE_LSTM, return_sequences=False, activation='elu', dropout=dropout, recurrent_dropout=dropout))(lstm_d_in)\n",
    "lstm_d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate and then Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_2/concat:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Concatenate()([lstm_q_out, lstm_d_out])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_2/cond/Merge:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dropout(dropout)(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_3/Elu:0' shape=(?, 4612) dtype=float32>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(DISC_HIDDEN_SIZE_DENSE,\n",
    "          activation='elu')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_4/Elu:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(1, activation='elu')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.00\n",
    "score = Lambda(lambda z: z / temperature, name='raw_score')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'score_1/Reshape:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = Reshape([-1], name='score')(score)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = Activation('softmax', name='prob')(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f274fec8198>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[sequence_input_q, sequence_input_d, reward, important_sampling], outputs=[prob])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_query (InputLayer)        (None, 20000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_doc (InputLayer)          (None, 20000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20000, 300)   6000000     input_query[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20000, 300)   6000000     input_doc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 20000, 128)   140160      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 20000, 128)   140160      embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 128)          74112       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 128)          74112       bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           bidirectional_6[0][0]            \n",
      "                                                                 bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4612)         1185284     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            4613        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "raw_score (Lambda)              (None, 1)            0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "score (Reshape)                 (None, 1)            0           raw_score[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 1)            0           score[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 13,618,441\n",
      "Trainable params: 1,618,441\n",
      "Non-trainable params: 12,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __loss(_reward, _important_sampling):\n",
    "    def _loss(y_true, y_pred):\n",
    "        log_action_prob = K.log(y_pred)\n",
    "        loss = - K.reshape(log_action_prob, [-1]) * K.reshape(_reward, [-1]) * K.reshape(_important_sampling, [-1])\n",
    "        loss = K.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f274fec8198>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=__loss(reward, important_sampling),\n",
    "                optimizer=adamw,\n",
    "                metrics=['accuracy'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_query_1:0' shape=(?, 20000) dtype=int32>, <tf.Tensor 'input_doc_1:0' shape=(?, 20000) dtype=int32>]\n",
      "Tensor(\"score_1/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"prob_1/Softmax:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inp = model.inputs[:2]\n",
    "print(inp)\n",
    "out_score = model.get_layer(\"score\").output\n",
    "print(out_score)\n",
    "out_prob = model.get_layer(\"prob\").output\n",
    "print(out_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize queries\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found 1247 unique tokens.\n",
      "Tokenize documents\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found 7709 unique tokens.\n",
      "Found 1 training data.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "WORKDIR = '/home/lukas/git-projects/lstm-irgan'\n",
    "DOCUMENTS_DIR = WORKDIR + '/data/wikiclir/dev.docs'  #'/data/example/documents/'\n",
    "QUERIES = WORKDIR + '/data/wikiclir/dev.queries' #'/data/example/queries.txt'\n",
    "LABELLED_DATA = WORKDIR + '/data/wikiclir/dev.qrel' #'/data/example/labelled_data.txt'\n",
    "\n",
    "def __get_documents():\n",
    "    path = DOCUMENTS_DIR\n",
    "    documents = {}\n",
    "    doc_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            documents[id] = text\n",
    "            doc_ids.append(id)\n",
    "    return documents, doc_ids\n",
    "\n",
    "\n",
    "def __get_queries():\n",
    "    path = QUERIES\n",
    "    queries = {}\n",
    "    query_ids = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\", 1)\n",
    "            id = int(values[0])\n",
    "            text = values[1]\n",
    "            queries[id] = text\n",
    "            query_ids.append(id)\n",
    "    return queries, query_ids\n",
    "\n",
    "\n",
    "def __get_ratings():\n",
    "    path = LABELLED_DATA\n",
    "    ratings = {}\n",
    "\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content[:100]:\n",
    "            values = line.split(\"\\t\")\n",
    "            query = int(values[0])\n",
    "            text = int(values[2])\n",
    "            rating = float(values[3])\n",
    "\n",
    "            if query in ratings.keys():\n",
    "                ratings[query][text] = rating\n",
    "            else:\n",
    "                ratings[query] = {text: rating}\n",
    "\n",
    "    return ratings\n",
    "\n",
    "\n",
    "def __filter_stop_words(texts, stop_words):\n",
    "    for i, text in enumerate(texts):\n",
    "        new_text = [word for word in text.split() if word not in stop_words]\n",
    "        texts[i] = ' '.join(new_text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def __init_tokenizer(text_data, max_sequence_length):\n",
    "    texts = list(text_data.values())\n",
    "    ids = list(text_data.keys())\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "    texts = __filter_stop_words(texts, stop_words)\n",
    "\n",
    "    # finally, vectorize the text samples into a 2D integer tensor\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    text_data_sequenced = {}\n",
    "    for i, text in enumerate(data):\n",
    "        text_data_sequenced[ids[i]] = text\n",
    "\n",
    "    return tokenizer, text_data_sequenced\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    documents_data, doc_ids = __get_documents()\n",
    "    queries_data, query_ids = __get_queries()\n",
    "    ratings_data = __get_ratings()\n",
    "\n",
    "    print('Tokenize queries')\n",
    "    tokenizer_q, queries_data = __init_tokenizer(queries_data, MAX_SEQUENCE_LENGTH)\n",
    "    print('Tokenize documents')\n",
    "    tokenizer_d, documents_data = __init_tokenizer(documents_data, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    print('Found %s training data.' % len(ratings_data))\n",
    "\n",
    "    return query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d\n",
    "\n",
    "query_ids, ratings_data, documents_data, queries_data, tokenizer_q, tokenizer_d = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   0,    0,    0, ...,   23, 3388, 1496], dtype=int32), array([   0,    0,    0, ...,  286,  591, 2098], dtype=int32), array([  0,   0,   0, ..., 169, 503, 188], dtype=int32)]\n",
      "[array([  0,   0,   0, ...,   2, 204,  28], dtype=int32), array([  0,   0,   0, ...,   2, 204,  28], dtype=int32), array([  0,   0,   0, ...,   2, 204,  28], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "docs = [value for key, value in list(documents_data.items())[:3]]\n",
    "key, value = list(queries_data.items())[0]\n",
    "queries = [value]*3\n",
    "print(docs)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00322543],\n",
       "        [0.00568766],\n",
       "        [0.00801787]], dtype=float32)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(train_data_queries, train_data_documents,):\n",
    "    inputs = model.inputs + [K.learning_phase()]\n",
    "    out = model.get_layer('score').output\n",
    "    functor = K.function(inputs, [out])\n",
    "    layer_outs = functor([train_data_queries, train_data_documents, 0.])\n",
    "    return layer_outs\n",
    "\n",
    "reward = get_score(queries, docs)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.],\n",
       "        [1.],\n",
       "        [1.]], dtype=float32)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prob(train_data_queries, train_data_documents,):\n",
    "    inputs = model.inputs + [K.learning_phase()]\n",
    "    out = model.get_layer('prob').output\n",
    "    functor = K.function(inputs, [out])\n",
    "    layer_outs = functor([train_data_queries, train_data_documents, 0.])\n",
    "    return layer_outs\n",
    "\n",
    "prob = get_prob(queries, docs)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(train_data_queries, train_data_documents, reward, important_sampling):\n",
    "    model.train_on_batch([train_data_queries, train_data_documents, reward, important_sampling], np.zeros([train_data_queries.shape[0]]))\n",
    "\n",
    "# choose data\n",
    "choose_queries = np.array(queries)\n",
    "choose_documents = np.array(docs)\n",
    "\n",
    "choose_queries = np.asarray(choose_queries)\n",
    "choose_documents = np.asarray(choose_documents)\n",
    "\n",
    "choose_reward = np.asarray([0.2, 0.5, 0.6])\n",
    "\n",
    "choose_is = np.asarray([0.2, 0.5, 0.6])\n",
    "\n",
    "train(choose_queries, choose_documents, choose_reward.reshape([-1]), choose_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
